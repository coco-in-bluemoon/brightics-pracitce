## 회귀(Regression)

### 회귀의 정의
- 독립변수로 종속변수의 값을 설명/예측한다
- 독립변수에 대한 선형 함수로 종속변수를 설명할 수 있다면 선형회귀(Linear Regression)라고 한다
- 사용하는 독립변수의 개수에 따라서 단순선형회귀(Simple Linear Regression), 다중선형회귀(Multiple Linear Regression)로 구분한다

### 단순선형회귀
- 독립변수 한 개로 종속 변수를 설명하는 선형회귀식을 구한다
- 선형회귀식으로 추정한 값과 실제 관측값의 차이를 최소로 만드는 모수를 추정한다
  - 차이 제곱 합에 대한 식을 미분하여서 최소로 만드는 모수 추정량 계산
<p align="center">
    <img width="300" height="" src="../images/equation_simple-regression.png"/>
</p>

- 제대로 만들어진 선형회귀식은 오차와 잔차가 비슷할 것이다
  - 오차: 관찰할 수 없는 모수
  - 잔차: 관측값에서 추정값을 뺀 값
  - 오차가 정규분포를 따른다는 가정하에 잔차제곱합으로 모수(오차)의 분산을 추정
<p align="center">
    <img width="150" height="" src="../images/equation_error.png"/>
</p>

### 다중선형회귀
- 독립변수 p개로 종속 변수를 설명하는 선형회귀식을 구한다
- 일반적으로 행렬로 표기한다.
<p align="center">
    <img width="300" height="" src="../images/equation_multiple-regression.png"/>
</p>

- 다중선형회귀에서는 다중공산성에 유의한다.
  - 다중공산성: 독립변수끼리 선형관계가 있어서 선형회귀식에 신뢰도를 떨어뜨린다
  - 다중공산성은 분산팽창계수(Variance Inflation Factor, VIF)이 큰 경우(5~10) 다중공산성이 있다고 가정한다
<p align="center">
    <img width="300" height="" src="../images/equation_vif.png"/>
</p>


### 선형회귀모형의 평가
**1. 결정계수와 수정된 결정계수**
- 결정계수: 자료가 가지는 전체적인 변동에 대해서 회귀식이 설명한느 변동의 비율. 
- 결정계수가 1에 가까울 수록 선형회귀식이 관측값이 실제값과 유사한 것을 의미한다
- 결정게수는 사용하는 독립변수의 개수가 늘어날수록 높은 값을 가질 확률이 커지므로 데이터의 개수와 사용하는 독립변수의 개수를 인자로 패널티를 부과하는 수정된 결정계수도 활용한다

<p align="center">
    <img width="300" height="" src="../images/equation_r.png"/>
</p>

**2. MSE와 MAE**
- MSE: 관측치와 추정치 사이에 오차제곱의 합. 자료의 스케일에 따라서 값이 달라질 수 있다.
- MAE: MSE는 오차가 클수록 패널티가 크기 때문에 일르 조정하는 MAE 사용.

<p align="center">
    <img width="200" height="" src="../images/equation_mse-mae.png"/>
</p>

### 선형회귀모형의 진단
- 선형회귀모형은 오차에 대한 가정이 필요하다
  1. 독립성: 오차는 서로 관련이 없다
  2. 등분산성: 자료의 퍼짐 정도는 독립변수의 수준과 관련이 없다
  3. 정규성: 자료들은 회귀모형을 기준으로 정규분포를 띈다.
- 오차를 대표하는 잔차를 산점도로 그려서 선형회귀모형을 진단한다.
  1. 산점도가 패턴이 없다 → 제대로된 선형회귀모형
  2. 산점도가 포물선이다 → 선형회귀모형에 이차항이 필요하다
  3. 산점도가 나팔모양이다 (앞/뒤로 갈수록 좁아지는 사다리꼴) → 등분산가정이 위배되었다
  4. 산점도가 직선모양이다 → 선형회귀모형에 새로운 독립변수가 필요하다 
- 이상치로 인해 선형회귀모형에 왜곡이 발생할 수 있으므로 분석가의 판단이 필요하다.

### 벌점화회귀
- 회귀식을 비롯한 모형 추정은 '비용함수를 최소화하는 모수'를 찾는 과정
  - 일반적으로 비용함수는 잔차들의 최소제곱합으로 구한다
- 과적합을 방지하기 위해 비용함수에 Regularization 항을 추가
  1. Ridge Regression (L2 Regularization)
  2. Lasso Regression (L1 Regularization)
  3. Elasticnet Regression = Ridge + Lasso

<p align="center">
    <img width="" height="" src="../images/equation_penalty.png"/>
</p>

- Regularization을 사용해서 모수(가중치)가 0에 가까운 작은 값으로 이끈다
  - 특히 Lasso Regression은 특정 모수(가중치)가 0이 될 수 있기 때문에 Feature Selection으로 사용할 수 있다

<p align="center">
    <img width="" height="" src="../images/figure_compare-l1-l2.png">
</p>

```
- 타원의 넓이가 가장 작은 선에서 모수(가중치, β)가 정해지는 것이 좋음
- Regularization이 추가됨에 따라서 원(L2) 또는 마름모(L1) 안에서 모수가 결정되어야한다
- 따라서 타원의 넓이가 점점 커지면서 원 또는 마름모와 닿을 때 모수(가중치, β)가 최종 모수이다
- 마름모(L1)인 경우 원(L2)인 경우와 다른게 한 쪽이 0이 될 가능성이 상대적으로 높다
    → Feature Selection
```
