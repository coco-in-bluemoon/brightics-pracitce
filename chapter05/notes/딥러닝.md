# 딥러닝

## ☕️ Recall
1. 딥러닝과 인공 신경망의 관계는 무엇인가?
2. 퍼셉트론의 구조는 무엇인가?
3. 딥러닝의 목적은 무엇이며, 해당 목적을 달성하기 위해 어떠한 방법으로 학습을 진행하는가?
4. 초기에 깊은 신경망에서 학습이 이루어지지 않는 문제를 해결하기 위해 어떠한 방법이 사용되었는가?
5. Gradient Vanishing 문제가 무엇이고 이를 해결하기 위한 방법은 무엇인가?
---

### 딥러닝 발전
- 딥러닝은 다층으로 구성된 인공 신경망을 의미한다.
- 인공 신경망은 퍼셉트론에서부터 시작하였으며 단층 퍼셉트론에서 다층 퍼셉트론을 사용한 학습이 가능해지면서 딥러닝이 발전한다
  - 퍼셉트론: n 개의 출력을 각각의 가중치로 입력 받아서 활성화 함수를 거쳐 출력을 하는 노드
- 다층으로 구성된 인공 신경망을 학습하기 위해서 경사 하강법과 오차 역전파가 활용된다
- 하지만 인공 신경망의 깊이가 깊어지면서 Gradient Vanishing 문제가 발생하였고 이를 해결하기 위해서 활성화 함수로 Sigmoid 대신에 ReLU를 사용해서 비선형성을 더했다
- 현재 신경망은 CNN, RNN, 생성 모델과 같은 방식으로 발전하고 있다.

**✔︎ 경사 하강법**
- 경사 하강법은 딥러닝을 학습하기 위한 방법이다.
- 딥러닝을 학습한다는 것은 비용 함수(cost function)을 최소화하는 가중치를 찾는 것이다.
- 경사 하강법은 대표적인 최적화 문제 해결 방법으로 미분값을 활용하여서 최적의 가중치를 점진적으로 찾아나간다.
  - 미분의 반대 방향으로 가중치를 조금씩 이동하면 극점에 이를 수 있다.

**✔︎ 오차 역전파**
- 오차 역전파는 깊이가 깊은 인공 신경망도 경사 하강법으로 통해 최적의 가중치를 찾을 수 있도록 도와주는 방법이다.
- 깊이가 깊은 인공 신경망에서 출력단과 먼 노드의 미분 값을 구하기 어렵다
- 편미분의 Chain Rule을 활용하여서 출력단과 가까운 곳에서의 미분(Error Signal)을 구하고 이를 이전 층(layer)로 역전파한다.
- 각 층에서는 다음 층으로부터 받은 미분(Error Signal)과 국소적 미분을 활용하여서 미분해당 층의 (Error Signal)을 구해서 다시 역전파한다.

**✔︎ Gradient Vanishing 문제와 ReLU**
- Gradient Vanishing 문제는 Sigmoid 활성화 함수의 특징으로 인해 오차 역전파되는 Error Signal의 갑이 0에 수렴하는 문제를 의미한다
- 오차 역전파되는 Error Signal이 0에 수렴하면서 가중치 갱신이 이루어지지 않는 문제가 일어난다.
- 이는 Sigmoid 활성화 함수의 미분값은 최대값이 약 0.4이고 큰 값과 작은 값에 대해서 0이 되기 때문에 반복적으로 곱하면 미분값이 0이 된다
- 이를 해결하기 위해서 활성화 함수가 Sigmoid 대신에 ReLU를 활용한다.
- ReLU 활성화 함수는 음수인 경우에 대해서는 반응하지 않지만(0 출력), 양수에 대해서는 있는 그대로 출력한다.
- 따라서 ReLU 활성화 함수는 양수 값에 대해서는 미분값이 1이 유지되기 때문에 반복적으로 곱해지더라도 Gradient Vanishing 문제 발생을 최소화할 수 있다.

--
## Quiz
1. 딥러닝에 대한 설명으로 적절하지 않은 것은?
   1. 인간의 신경계에서 영감을 얻은 인공 신경망에 대한 연구가 시작되었다 (O)
   2. 최근 컴퓨팅 속도와 데이터의 폭발적 증가로 전성기를 맞이하고 있다 (O)
   3. 기본 구조인 퍼셉트론은 하나의 입력만 받을 수 있다. (X, n 개의 입력과 1개의 출력)
   4. 퍼셉트론을 여러 계층으로 쌓은 다층 퍼셉트론은 비선형 모델링이 가능하다 (O)
2. 딥러닝을 학습하기 위해 사용하는 하이퍼파라미터에 대한 설명으로 옳은 것은?
   1. 손실 함수(loss): 학습 시 예측 값이 실제 값과 얼마나 가까운지 판별하는 지표이다. (O)
   2. 평가 함수(metrics): 학습 시 조기 종료를 위한 지표로 사용 가능하다. (X, 조기 종료는 손실 함수를 가지고 판단한다)
   3. 배치 사이즈(batch size): 노드 사이의 가중치를 업데이트 할 때 방법을 결정한다. (X)
   4. 에포크(epoch): 학습 시 한 번에 보는 데이터의 크기를 말한다. (X, 에포크는 전체 데이터를 기반으로 학습이 이루어진 횟수이다)
3. 신경망 모델의 주요 특성인 전이 학습(transfer learning)에 대한 설명이 아닌 것은?
   1. 사전에 잘 학습된 모델을 이용하여 모델의 성능을 올릴 수 있다. (O)
   2. 음성 인식 등 다양한 분야에서 사용되고 있다 (O)
   3. 시간적, 자원적으로 큰 장점을 갖는다 (O)
   4. 해결해야하는 문제의 특성이 다른 경우에도 사용 가능하다 (X, 유사한 과제에 적용된다)
4. 컨볼루션 신경망(CNN)의 특성으로 가장 적합한 것을 고르시오.
   1. 학습 데이터가 부족한 경우 유사한 새로운 데이터를 생성할 수 있다.(X, 셍성 모델)
   2. 학습 데이터를 설명하기 위한 중요한 특징을 모델이 찾아낸다. (O)
   3. 순서가 의미 있는 시계열 데이터의 패턴을 학습하는데 용이하다 (X, RNN)
   4. 확률 분포를 이용하여 학습을 수행한다. (X, 생성 모델)
5. 순환 신경망(RNN)의 대표적인 레이어가 아닌 것은 무엇인가?
   1. RNN
   2. GRU
   3. Pooling (X, CNN)
   4. LSTM
6. GAN(generative adversarial network) 모델에 대해 틀린 설명을 고르시오.
   1. 대표적이 생성 모델(generative model)이다
   2. 생성자(generator)와 구분자(discriminator)로 구성되어 있다
   3. 적대적인 학습(adversarial learning)을 수행하는 구조이다
   4. 구분자가 완전히 신규 데이터를 분류하면 학습을 종료한다. (X, 생성자의 생성 결과를 구분자가 진짜와 구분하기 어렵도록 학습을 진행한다)